{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "1. Split dataset into sentences\n",
    "2. Remove punctuation\n",
    "3. All letters lowercase\n",
    "4. Split sentences into words\n",
    "5. Create a vocabulary (all known words of the ML model)\n",
    "6. Tokenize words\n",
    "7. Add and tags in vocabulary ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78, 78)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "\n",
    "with open('data/input/input.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if line[0] == 'I':\n",
    "            questions.append(line.strip())\n",
    "        else:\n",
    "            answers.append(line.strip())\n",
    "\n",
    "answers = list(filter(None, answers))\n",
    "len(questions), len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 'Intrebarea' si 'Raspunsul'\n",
    "import re \n",
    "\n",
    "def remove_preposition(text):\n",
    "    q_regex = 'Intrebarea\\ [0-9]+\\:\\ '\n",
    "    a_regex = 'Raspuns\\ [0-9]+\\:\\ '\n",
    "    \n",
    "    if 'Intrebarea' in text:\n",
    "        text = re.sub(q_regex, '', text)\n",
    "    else:\n",
    "        text = re.sub(a_regex, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuationfree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "\n",
    "def remove_stopwords(text, path_to_stopwords):\n",
    "    stopwords = open(path_to_stopwords).readline().split(',')\n",
    "    return [x for x in text if x not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all lowercase\n",
    "def to_lower(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into words\n",
    "def split_words(text):\n",
    "    tokens = re.split(' ',text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import RomanianStemmer\n",
    "\n",
    "def stemming(text):\n",
    "    snow_stemmer = RomanianStemmer()\n",
    "    return [snow_stemmer.stem(word) for word in text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting together the pipeline\n",
    "\n",
    "def preprocessing_pipeline(_list):\n",
    "    new_list = []\n",
    "    for text in _list:\n",
    "        text = remove_preposition(text)\n",
    "        text = remove_punctuation(text)\n",
    "        text = split_words(text)\n",
    "        text = remove_stopwords(text, 'data/input/stopwords-ro.txt')\n",
    "        text = list(filter(None, text))\n",
    "        text = [to_lower(x) for x in text]\n",
    "        text = stemming(text)\n",
    "        text = ['<start>'] + text \n",
    "        text.append('<end>')\n",
    "        new_list.append(text)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<START>', 'se', 'fac', 'internshipur', 'individual', '<END>'],\n",
       " ['<START>', 'pot', 'evalu', 'internship', 'onlin', '<END>'],\n",
       " ['<START>', 'este', 'internship', 'gratis', '<END>'],\n",
       " ['<START>', 'cin', 'aplic', '<END>'],\n",
       " ['<START>', 'car', 'indatorir', 'respons', 'accept', '<END>'],\n",
       " ['<START>',\n",
       "  'fapt',\n",
       "  'moment',\n",
       "  'aflu',\n",
       "  'republ',\n",
       "  'moldov',\n",
       "  'influent',\n",
       "  'etap',\n",
       "  'select',\n",
       "  'interviur',\n",
       "  'hr',\n",
       "  'tehnic',\n",
       "  '<END>'],\n",
       " ['<START>', 'exist', 'opțiun', 'intership', 'devrem', 'apriliem', '<END>'],\n",
       " ['<START>', 'internship', 'loc', 'onlin', 'sed', 'compan', '<END>'],\n",
       " ['<START>', 'cum', 'internship', 'onlineofflin', '<END>'],\n",
       " ['<START>', 'car', 'progr', 'zilnic', 'internship', '<END>'],\n",
       " ['<START>',\n",
       "  'e',\n",
       "  'posibil',\n",
       "  'incep',\n",
       "  'internship',\n",
       "  'devrem',\n",
       "  'iulieaugust',\n",
       "  '<END>'],\n",
       " ['<START>',\n",
       "  'am',\n",
       "  'vazut',\n",
       "  'intership',\n",
       "  'student',\n",
       "  'persoan',\n",
       "  'sas',\n",
       "  'schimb',\n",
       "  'carier',\n",
       "  'caz',\n",
       "  'vreau',\n",
       "  'sam',\n",
       "  'schimb',\n",
       "  'carier',\n",
       "  'treb',\n",
       "  'astept',\n",
       "  'pan',\n",
       "  'iul',\n",
       "  'august',\n",
       "  'put',\n",
       "  'incep',\n",
       "  '<END>'],\n",
       " ['<START>',\n",
       "  'internship',\n",
       "  'desfasur',\n",
       "  'remot',\n",
       "  'se',\n",
       "  'echival',\n",
       "  'stagiu',\n",
       "  'practic',\n",
       "  'facult',\n",
       "  '<END>'],\n",
       " ['<START>',\n",
       "  'exist',\n",
       "  'număr',\n",
       "  'limit',\n",
       "  'participăr',\n",
       "  'internship',\n",
       "  'persoan',\n",
       "  '<END>'],\n",
       " ['<START>', 'car', 'durat', 'internship', '<END>'],\n",
       " ['<START>', 'se', 'stiu', 'zil', 'incep', 'internship', '<END>'],\n",
       " ['<START>', 'pe', 'perioad', 'internship', '<END>'],\n",
       " ['<START>', 'internship', 'desfasoar', 'suceav', 'se', 'remot', '<END>'],\n",
       " ['<START>', 'intern', 'scrie', 'cod', 'intră', 'producț', '<END>'],\n",
       " ['<START>', 'put', 'particip', 'tehnolog', 'test', '<END>'],\n",
       " ['<START>', 'intrebatr', '21', 'pot', 'aplic', 'android', '<END>'],\n",
       " ['<START>', 'se', 'internship', 'afar', 'perioade', 'anunt', '<END>'],\n",
       " ['<START>', 'este', 'reloc', 'suceav', 'sau', 'onlin', '<END>'],\n",
       " ['<START>', 'va', 'internship', 'echip', 'individual', '<END>'],\n",
       " ['<START>',\n",
       "  'ce',\n",
       "  'sfatur',\n",
       "  'viaț',\n",
       "  'carier',\n",
       "  'miaț',\n",
       "  'put',\n",
       "  'ofer',\n",
       "  'fiind',\n",
       "  'lipsit',\n",
       "  'experienț',\n",
       "  'încep',\n",
       "  'carier',\n",
       "  '<END>'],\n",
       " ['<START>', 'ce', 'standard', 'treb', 'respect', 'candid', 'ideal', '<END>'],\n",
       " ['<START>', 'ce', 'calitat', 'import', 'excel', 'rol', '<END>'],\n",
       " ['<START>',\n",
       "  'ce',\n",
       "  'rezult',\n",
       "  'așteptaț',\n",
       "  'student',\n",
       "  'perioad',\n",
       "  'intership',\n",
       "  '<END>'],\n",
       " ['<START>',\n",
       "  'as',\n",
       "  'curios',\n",
       "  'opin',\n",
       "  'mentor',\n",
       "  'internship',\n",
       "  'cunostint',\n",
       "  'dest',\n",
       "  'solid',\n",
       "  'baz',\n",
       "  'limbaj',\n",
       "  'ccc',\n",
       "  'concept',\n",
       "  'program',\n",
       "  'utiliz',\n",
       "  'diver',\n",
       "  'aplic',\n",
       "  'reprezint',\n",
       "  'suficient',\n",
       "  'incat',\n",
       "  'student',\n",
       "  'fac',\n",
       "  'internship',\n",
       "  'includ',\n",
       "  'fapt',\n",
       "  'respect',\n",
       "  'student',\n",
       "  'sar',\n",
       "  'descurc',\n",
       "  'test',\n",
       "  'tehnic',\n",
       "  'interviu',\n",
       "  'baz',\n",
       "  'concept',\n",
       "  'precum',\n",
       "  'oop',\n",
       "  'facult',\n",
       "  'cursur',\n",
       "  'studiat',\n",
       "  'document',\n",
       "  'studiat',\n",
       "  'timp',\n",
       "  'liber',\n",
       "  'proiect',\n",
       "  '<END>'],\n",
       " ['<START>',\n",
       "  'car',\n",
       "  'top',\n",
       "  '3',\n",
       "  'calitat',\n",
       "  'treb',\n",
       "  'dea',\n",
       "  'dovad',\n",
       "  'candid',\n",
       "  '<END>'],\n",
       " ['<START>',\n",
       "  'am',\n",
       "  'put',\n",
       "  'ataș',\n",
       "  'portofoliu',\n",
       "  'făcut',\n",
       "  'internship',\n",
       "  '<END>'],\n",
       " ['<START>',\n",
       "  'car',\n",
       "  'principal',\n",
       "  'calitat',\n",
       "  'cautat',\n",
       "  'candid',\n",
       "  'internship',\n",
       "  '<END>'],\n",
       " ['<START>',\n",
       "  'car',\n",
       "  'domen',\n",
       "  'interes',\n",
       "  'treb',\n",
       "  'perform',\n",
       "  'eventual',\n",
       "  'veder',\n",
       "  'introduc',\n",
       "  'portofol',\n",
       "  'profesional',\n",
       "  '<END>'],\n",
       " ['<START>',\n",
       "  'îmi',\n",
       "  'puteț',\n",
       "  'sfat',\n",
       "  'pregăt',\n",
       "  'internship',\n",
       "  'benefic',\n",
       "  '100',\n",
       "  'cunoștinț',\n",
       "  'experient',\n",
       "  'ofer',\n",
       "  '<END>'],\n",
       " ['<START>',\n",
       "  'se',\n",
       "  'rezolv',\n",
       "  'test',\n",
       "  'tehnic',\n",
       "  'limbaj',\n",
       "  'respect',\n",
       "  'list',\n",
       "  '<END>'],\n",
       " ['<START>', 'tehnolog', 'testat', 'influent', 'durat', 'internship', '<END>'],\n",
       " ['<START>', 'as', 'put', 'testat', 'tehnolog', '<END>'],\n",
       " ['<START>', 'cand', 'treb', 'test', 'tehnic', '<END>'],\n",
       " ['<START>',\n",
       "  'limbaj',\n",
       "  'program',\n",
       "  'aleg',\n",
       "  'test',\n",
       "  'influenț',\n",
       "  'perioad',\n",
       "  'internship',\n",
       "  '<END>'],\n",
       " ['<START>', 'la', 'proiect', 'put', 'lucr', '<END>'],\n",
       " ['<START>',\n",
       "  'dac',\n",
       "  'limbaj',\n",
       "  'principal',\n",
       "  'cunosc',\n",
       "  'binepython',\n",
       "  'afla',\n",
       "  'list',\n",
       "  '<END>'],\n",
       " ['<START>', 'la', 'proiect', 'lucr', 'intern', '<END>'],\n",
       " ['<START>',\n",
       "  'dat',\n",
       "  'fiind',\n",
       "  'fapt',\n",
       "  'mias',\n",
       "  'dor',\n",
       "  'propun',\n",
       "  'invat',\n",
       "  'swift',\n",
       "  'oar',\n",
       "  'internship',\n",
       "  'lucrur',\n",
       "  'legat',\n",
       "  'swift',\n",
       "  'oar',\n",
       "  'departament',\n",
       "  'dea',\n",
       "  'swift',\n",
       "  'firm',\n",
       "  'assist',\n",
       "  '<END>'],\n",
       " ['<START>',\n",
       "  'am',\n",
       "  'vazut',\n",
       "  'avet',\n",
       "  'dou',\n",
       "  'proiect',\n",
       "  'folos',\n",
       "  'tehnolog',\n",
       "  'blockchain',\n",
       "  'dezvolt',\n",
       "  'blockchain',\n",
       "  'propriu',\n",
       "  'folos',\n",
       "  'solut',\n",
       "  'existent',\n",
       "  'gen',\n",
       "  'platform',\n",
       "  'ethereum',\n",
       "  '<END>'],\n",
       " ['<START>', 'car', 'proiect', 'cum', 'lucr', 'echip', 'indiviual', '<END>'],\n",
       " ['<START>',\n",
       "  'cum',\n",
       "  'afect',\n",
       "  'situat',\n",
       "  'actual',\n",
       "  'legat',\n",
       "  'pandem',\n",
       "  'covid19',\n",
       "  '<END>'],\n",
       " ['<START>',\n",
       "  'cum',\n",
       "  'arat',\n",
       "  'activ',\n",
       "  'coleg',\n",
       "  'lucreaz',\n",
       "  'test',\n",
       "  'manual',\n",
       "  'autom',\n",
       "  '<END>'],\n",
       " ['<START>',\n",
       "  'pe',\n",
       "  'viitor',\n",
       "  'exist',\n",
       "  'locur',\n",
       "  'internship',\n",
       "  'grafic',\n",
       "  '3d2d',\n",
       "  '<END>'],\n",
       " ['<START>', 'ce', 'săm', 'ofer', 'internship', 'assist', '<END>'],\n",
       " ['<START>', 'de', 'aleg', 'assist', 'altă', 'compan', 'softw', '<END>'],\n",
       " ['<START>', 'car', 'oportun', 'dezvolt', 'carier', '<END>'],\n",
       " ['<START>', 'car', 'profil', 'angaj', 'succes', 'compan', '<END>'],\n",
       " ['<START>',\n",
       "  'cunoștinț',\n",
       "  'dobând',\n",
       "  'internship',\n",
       "  'ajut',\n",
       "  'admit',\n",
       "  'facult',\n",
       "  '<END>'],\n",
       " ['<START>',\n",
       "  'ce',\n",
       "  'oportunităț',\n",
       "  'exist',\n",
       "  'cadr',\n",
       "  'compan',\n",
       "  'urma',\n",
       "  'internship',\n",
       "  '<END>'],\n",
       " ['<START>',\n",
       "  'din',\n",
       "  'perspect',\n",
       "  'încheier',\n",
       "  'succes',\n",
       "  'acestor',\n",
       "  'form',\n",
       "  'pregăt',\n",
       "  'posibil',\n",
       "  'inițier',\n",
       "  'colaborăr',\n",
       "  'profesional',\n",
       "  'ulterioar',\n",
       "  '<END>'],\n",
       " ['<START>',\n",
       "  'exist',\n",
       "  'posibil',\n",
       "  'termin',\n",
       "  'internship',\n",
       "  'rămân',\n",
       "  'angaj',\n",
       "  'cadr',\n",
       "  'firme',\n",
       "  '<END>'],\n",
       " ['<START>', 'ce', 'invat', '<END>'],\n",
       " ['<START>', 'cum', 'aflu', 'selection', '<END>'],\n",
       " ['<START>', 'trebui', 'neapar', 'student', 'facult', 'mention', '<END>'],\n",
       " ['<START>', 'dac', 'termin', 'an', 'mic', 'aplic', '<END>'],\n",
       " ['<START>', 'dac', 'ales', 'plec', 'internship', '<END>'],\n",
       " ['<START>',\n",
       "  'voi',\n",
       "  'prim',\n",
       "  'scrisoar',\n",
       "  'recomand',\n",
       "  'final',\n",
       "  'progr',\n",
       "  'internship',\n",
       "  '<END>'],\n",
       " ['<START>', 'pot', 'aplic', 'oportun', 'internship', '<END>'],\n",
       " ['<START>', 'internshipur', 'ofer', 'platit', '<END>'],\n",
       " ['<START>', 'cin', 'aplic', 'assist', 'academy', '<END>'],\n",
       " ['<START>', 'dac', 'softw', '<END>'],\n",
       " ['<START>', 'este', 'limb', 'englez', 'musthav', 'industr', '<END>'],\n",
       " ['<START>', 'cin', 'mentor', '<END>'],\n",
       " ['<START>', 'sunt', 'program', 'accesib', 'onlin', '<END>'],\n",
       " ['<START>', 'câț', 'coleg', '<END>'],\n",
       " ['<START>', 'cum', 'aplic', '<END>'],\n",
       " ['<START>', 'ce', 'acte', 'aplic', '<END>'],\n",
       " ['<START>', 'pot', 'sam', 'aleg', 'doresc', 'fac', 'internship', '<END>'],\n",
       " ['<START>', 'put', 'aleg', 'internship', 'onlin', 'offlin', '<END>'],\n",
       " ['<START>', 'cum', 'inscrie', 'ne', 'trimitet', 'mail', 'acte', '<END>'],\n",
       " ['<START>',\n",
       "  'nu',\n",
       "  'facult',\n",
       "  'domeniu',\n",
       "  'pasion',\n",
       "  'domeniu',\n",
       "  'de',\n",
       "  'incep',\n",
       "  '<END>'],\n",
       " ['<START>', 'pot', 'aplic', 'internship', '<END>'],\n",
       " ['<START>', 'pot', 'aleg', 'proiect', 'doresc', 'lucr', '<END>']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_pipeline(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [98]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# creating the Tokenizer and the vocabulary\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ----this is deprecated for now\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# new_texts = ['cine poate participa la internship?']\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# print(tokenizer.texts_to_sequences(new_texts))\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mword_index)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# creating the Tokenizer and the vocabulary\n",
    "\n",
    "# ----this is deprecated for now\n",
    "\n",
    "# inp_lang_tokenizer =  tf.keras.preprocessing.text.Tokenizer()\n",
    "# targ_lang_tokenizer =  tf.keras.preprocessing.text.Tokenizer()\n",
    "\n",
    "# lang = questions + answers\n",
    "# tokenizer.fit_on_texts(text_corpus)\n",
    "\n",
    "# new_texts = ['cine poate participa la internship?']\n",
    "# print(tokenizer.texts_to_sequences(new_texts))\n",
    "# print(tokenizer.word_index) # the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've decided to create a Seq-to-Seq model. This type of model requires an Encoder and a Decoder. The example is from the [TensorFlow official documentation](https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset:\n",
    "    def __init__(self):\n",
    "        self.inp_lang_tokenizer = None\n",
    "        self.targ_lang_tokenizer = None\n",
    "        \n",
    "    def preprocess_sentence(self, sentence):\n",
    "        text = remove_preposition(sentence)\n",
    "        text = remove_punctuation(text)\n",
    "        text = split_words(text)\n",
    "        text = remove_stopwords(text, 'data/input/stopwords-ro.txt')\n",
    "        text = list(filter(None, text))\n",
    "        text = [to_lower(x) for x in text]\n",
    "        text = stemming(text)\n",
    "        text = ['<START>'] + text \n",
    "        text.append('<END>')\n",
    "\n",
    "    def tokenize(self, lang):\n",
    "        # lang = list of sentences in a language\n",
    "\n",
    "        # print(len(lang), \"example sentence: {}\".format(lang[0]))\n",
    "        lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>')\n",
    "        lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "        ## tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn) \n",
    "        ## to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)\n",
    "        tensor = lang_tokenizer.texts_to_sequences(lang) \n",
    "\n",
    "        ## tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences \n",
    "        ## and pads the sequences to match the longest sequences in the given input\n",
    "        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "        return tensor, lang_tokenizer\n",
    "\n",
    "    def load_dataset(self):\n",
    "        # creating cleaned input, output pairs\n",
    "        targ_lang, inp_lang = questions, answers\n",
    "\n",
    "        input_tensor, inp_lang_tokenizer = self.tokenize(inp_lang)\n",
    "        target_tensor, targ_lang_tokenizer = self.tokenize(targ_lang)\n",
    "\n",
    "        return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
    "\n",
    "    def call(self, BUFFER_SIZE, BATCH_SIZE):\n",
    "        input_tensor, target_tensor, self.inp_lang_tokenizer, self.targ_lang_tokenizer = self.load_dataset()\n",
    "\n",
    "        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))\n",
    "        train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\n",
    "        val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "        return train_dataset, val_dataset, self.inp_lang_tokenizer, self.targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 32000\n",
    "BATCH_SIZE = 5\n",
    "# Let's limit the #training examples for faster training\n",
    "\n",
    "dataset_creator = QADataset()\n",
    "train_dataset, val_dataset, inp_lang, targ_lang = dataset_creator.call(BUFFER_SIZE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 10:38:12.000663: W tensorflow/core/data/root_dataset.cc:200] Optimization loop failed: CANCELLED: Operation was cancelled\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([5, 108]), TensorShape([5, 81]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "max_length_input = example_input_batch.shape[1]\n",
    "max_length_output = example_target_batch.shape[1]\n",
    "num_examples = 100 # an arbitrary value that can be changed later\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "steps_per_epoch = num_examples//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length_inp, max_length_out, vocab_size_questions, vocab_size_answers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(108, 81, 683, 507)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"max_length_inp, max_length_out, vocab_size_questions, vocab_size_answers\")\n",
    "max_length_input, max_length_output, vocab_inp_size, vocab_tar_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### \n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    ##-------- LSTM layer in Encoder ------- ##\n",
    "    self.lstm_layer = tf.keras.layers.LSTM(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, h, c = self.lstm_layer(x, initial_state = hidden)\n",
    "    return output, h, c\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (5, 108, 1024)\n",
      "Encoder h vecotr shape: (batch size, units) (5, 1024)\n",
      "Encoder c vector shape: (batch size, units) (5, 1024)\n"
     ]
    }
   ],
   "source": [
    "## Test Encoder Stack\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder h vector shape: (batch size, units) {}'.format(sample_h.shape))\n",
    "print ('Encoder c vector shape: (batch size, units) {}'.format(sample_c.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, attention_type='luong'):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.attention_type = attention_type\n",
    "\n",
    "    # Embedding Layer\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    #Final Dense layer on which softmax will be applied\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # Define the fundamental cell for decoder recurrent structure\n",
    "    self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\n",
    "\n",
    "\n",
    "\n",
    "    # Sampler\n",
    "    self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "    # Create attention mechanism with memory = None\n",
    "    self.attention_mechanism = self.build_attention_mechanism(self.dec_units, \n",
    "                                                              None, self.batch_sz*[max_length_input], self.attention_type)\n",
    "\n",
    "    # Wrap attention mechanism with the fundamental rnn cell of decoder\n",
    "    self.rnn_cell = self.build_rnn_cell(batch_sz)\n",
    "\n",
    "    # Define the decoder with respect to fundamental rnn cell\n",
    "    self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)\n",
    "\n",
    "\n",
    "  def build_rnn_cell(self, batch_sz):\n",
    "    rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell, \n",
    "                                  self.attention_mechanism, attention_layer_size=self.dec_units)\n",
    "    return rnn_cell\n",
    "\n",
    "  def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='luong'):\n",
    "    # ------------- #\n",
    "    # typ: Which sort of attention (Bahdanau, Luong)\n",
    "    # dec_units: final dimension of attention outputs \n",
    "    # memory: encoder hidden states of shape (batch_size, max_length_input, enc_units)\n",
    "    # memory_sequence_length: 1d array of shape (batch_size) with every element set to max_length_input (for masking purpose)\n",
    "\n",
    "    if(attention_type=='bahdanau'):\n",
    "      return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
    "    else:\n",
    "      return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
    "\n",
    "  def build_initial_state(self, batch_sz, encoder_state, Dtype):\n",
    "    decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)\n",
    "    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
    "    return decoder_initial_state\n",
    "\n",
    "\n",
    "  def call(self, inputs, initial_state):\n",
    "    x = self.embedding(inputs)\n",
    "    outputs, _, _ = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_sz*[max_length_output-1])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Outputs Shape:  (5, 80, 507)\n"
     ]
    }
   ],
   "source": [
    "# Test decoder stack\n",
    "\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, 'luong')\n",
    "sample_x = tf.random.uniform((BATCH_SIZE, max_length_output))\n",
    "decoder.attention_mechanism.setup_memory(sample_output)\n",
    "initial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c], tf.float32)\n",
    "\n",
    "\n",
    "sample_decoder_outputs = decoder(sample_x, initial_state)\n",
    "\n",
    "print(\"Decoder Outputs Shape: \", sample_decoder_outputs.rnn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  # real shape = (BATCH_SIZE, max_length_output)\n",
    "  # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\n",
    "  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "  loss = cross_entropy(y_true=real, y_pred=pred)\n",
    "  mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n",
    "  mask = tf.cast(mask, dtype=loss.dtype)  \n",
    "  loss = mask* loss\n",
    "  loss = tf.reduce_mean(loss)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_dir = './data/output'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
    "\n",
    "\n",
    "    dec_input = targ[ : , :-1 ] # Ignore <end> token\n",
    "    real = targ[ : , 1: ]         # ignore <start> token\n",
    "\n",
    "    # Set the AttentionMechanism object with encoder_outputs\n",
    "    decoder.attention_mechanism.setup_memory(enc_output)\n",
    "\n",
    "    # Create AttentionWrapperState as initial_state for decoder\n",
    "    decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\n",
    "    pred = decoder(dec_input, decoder_initial_state)\n",
    "    logits = pred.rnn_output\n",
    "    loss = loss_function(real, logits)\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 10:41:09.626536: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-03-30 10:41:09.629146: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-03-30 10:41:09.741700: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2022-03-30 10:41:09.919733: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-03-30 10:41:41.541545: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.7137\n",
      "Epoch 1 Loss 0.6537\n",
      "Time taken for 1 epoch 307.77152395248413 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.6936\n",
      "Epoch 2 Loss 0.5870\n",
      "Time taken for 1 epoch 303.0016119480133 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.7554\n",
      "Epoch 3 Loss 0.5622\n",
      "Time taken for 1 epoch 308.2094042301178 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.6721\n",
      "Epoch 4 Loss 0.5478\n",
      "Time taken for 1 epoch 306.85611510276794 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.5752\n",
      "Epoch 5 Loss 0.5190\n",
      "Time taken for 1 epoch 314.1287651062012 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.8160\n",
      "Epoch 6 Loss 0.4990\n",
      "Time taken for 1 epoch 301.7817573547363 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.6610\n",
      "Epoch 7 Loss 0.4898\n",
      "Time taken for 1 epoch 307.19200015068054 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.2488\n",
      "Epoch 8 Loss 0.4716\n",
      "Time taken for 1 epoch 264.7693381309509 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.6134\n",
      "Epoch 9 Loss 0.4607\n",
      "Time taken for 1 epoch 281.0602502822876 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.6829\n",
      "Epoch 10 Loss 0.4439\n",
      "Time taken for 1 epoch 307.8385169506073 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "import time\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "  # print(enc_hidden[0].shape, enc_hidden[1].shape)\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sentence(sentence):\n",
    "#   sentence = dataset_creator.preprocess_sentence(sentence)\n",
    "\n",
    "  text = remove_preposition(sentence)\n",
    "  text = remove_punctuation(text)\n",
    "  text = split_words(text)\n",
    "  text = remove_stopwords(text, 'data/input/stopwords-ro.txt')\n",
    "  text = list(filter(None, text))\n",
    "  text = [to_lower(x) for x in text]\n",
    "  text = stemming(text)\n",
    "    \n",
    "  print(inp_lang.word_index)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in text]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                          maxlen=max_length_input,\n",
    "                                                          padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "  inference_batch_size = inputs.shape[0]\n",
    "  result = ''\n",
    "\n",
    "  enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
    "  enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
    "\n",
    "  dec_h = enc_h\n",
    "  dec_c = enc_c\n",
    "\n",
    "  start_tokens = tf.fill([inference_batch_size], targ_lang.word_index['<start>'])\n",
    "  end_token = targ_lang.word_index['<end>']\n",
    "\n",
    "  greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "\n",
    "  # Instantiate BasicDecoder object\n",
    "  decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc)\n",
    "  # Setup Memory in decoder stack\n",
    "  decoder.attention_mechanism.setup_memory(enc_out)\n",
    "\n",
    "  # set decoder_initial_state\n",
    "  decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\n",
    "\n",
    "\n",
    "  ### Since the BasicDecoder wraps around Decoder's rnn cell only, you have to ensure that the inputs to BasicDecoder \n",
    "  ### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \n",
    "  ### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] and pass this callabble to BasicDecoder's call() function\n",
    "\n",
    "  decoder_embedding_matrix = decoder.embedding.variables[0]\n",
    "\n",
    "  outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, end_token= end_token, initial_state=decoder_initial_state)\n",
    "  return outputs.sample_id.numpy()\n",
    "\n",
    "def predict(sentence):\n",
    "  result = evaluate_sentence(sentence)\n",
    "  print(result)\n",
    "  result = targ_lang.sequences_to_texts(result)\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x169455f10>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'de': 2, 'raspuns': 3, 'la': 4, 'in': 5, 'sa': 6, 'si': 7, 'un': 8, 'pe': 9, 'se': 10, 'pentru': 11, 'vei': 12, 'care': 13, 'a': 14, 'internship': 15, 'o': 16, 'din': 17, 'daca': 18, 'fi': 19, 'cu': 20, 'ar': 21, 'nu': 22, 'mai': 23, 'vor': 24, 'este': 25, 'internship.': 26, 'da,': 27, 'internship-urile': 28, 'sediul': 29, 'te': 30, 'poti': 31, 'sau': 32, 'ai': 33, 'cat': 34, 'trebui': 35, 'despre': 36, 'au': 37, 'loc': 38, 'internship-ul': 39, 'aplica': 40, 'putea': 41, 'doua': 42, 'unui': 43, 'limbaj': 44, 'parcursul': 45, 'assist.': 46, 'ce': 47, 'exista': 48, 'desfasoara': 49, 'practica': 50, 'lucra': 51, 'echipa,': 52, 'sunt': 53, 'prin': 54, 'pot': 55, 'concluzie': 56, 'urma': 57, 'ca': 58, 'are': 59, 'cadrul': 60, 'il': 61, 'iti': 62, 'cunostinte': 63, 'cel': 64, 'experienta': 65, 'vom': 66, 'assist': 67, 'cum': 68, 'avea': 69, 'dar': 70, 'studiu': 71, 'va': 72, 'individual,': 73, 'grup': 74, 'ideal': 75, 'proiect': 76, 'internship-ului': 77, 'proiecte': 78, 'posibilitatea': 79, 'noastre': 80, 'adresa': 81, 'studenti': 82, 'orice': 83, 'persoane': 84, 'cele': 85, 'nu,': 86, 'varianta': 87, 'ajunge': 88, 'aceasta': 89, 'evaluarii.': 90, 'obtine': 91, 'durata': 92, 'luna': 93, 'iar': 94, 'invatare': 95, 'creat': 96, 'special': 97, 'job': 98, 'dezvolta': 99, 'fie': 100, 'usv,': 101, 'programare.': 102, 'acesta': 103, 'deja': 104, 'programare,': 105, 'lucru': 106, 'tehnologia': 107, 'folosi': 108, 'solutii': 109, 'studia': 110, 'resursele': 111, 'individuale': 112, 'online,': 113, 'partea': 114, 'elevi': 115, 'sustinut': 116, 'examenul': 117, 'bacalaureat,': 118, 'doresc': 119, 'reorienteze': 120, 'punct': 121, 'vedere': 122, 'profesional.': 123, 'importante': 124, 'participarea': 125, 'selectie': 126, 'functie': 127, 'selectat': 128, 'viitor': 129, 'ale': 130, 'iulie': 131, 'testare': 132, 'multa': 133, 'le': 134, 'asemenea,': 135, 'cunostintele': 136, 'candidatul': 137, 'student': 138, 'pasionat': 139, 'detina': 140, 'putin': 141, 'personale.': 142, 'alte': 143, 'internship-ului.': 144, 'programarii,': 145, 'doresti': 146, 'ta': 147, 'afla': 148, 'propune': 149, 'fost': 150, 'unor': 151, 'oportunitati': 152, 'platit': 153, 'studiu,': 154, 'dobandite': 155, 'accesibile': 156, 'inscriere': 157, 'disponibil': 158, 'https://assist-software.net/internship.': 159, 'tot': 160, 'anului.': 161, 'materialelor': 162, 'oferite': 163, 'noi,': 164, 'mod': 165, 'toate': 166, 'etapele': 167, 'desfasura': 168, 'firmei.': 169, 'perioada': 170, 'facultate.': 171, 'neaparat,': 172, 'dupa': 173, 'unul': 174, 'internship-uri,': 175, 'ajunga': 176, 'privinta': 177, 'posibil': 178, 'companie.': 179, 'saptamani': 180, '4': 181, 'am': 182, 'prima': 183, 'sesiune': 184, 'august.': 185, 'participantii': 186, 'android': 187, 'una': 188, 'tehnologiile': 189, 'proiectul': 190, 'nou': 191, 'personale,': 192, 'ajute': 193, 'atributii': 194, 'perseverenta,': 195, 'activitatea': 196, 'alt': 197, 'fii': 198, 'spre': 199, 'fiecare': 200, 'testul': 201, 'testat': 202, 'servi': 203, 'metoda': 204, 'initiala': 205, 'selectie.': 206, 'probabil': 207, 'acea': 208, 'tehnologie': 209, '2': 210, 'testare.': 211, 'informat': 212, 'acest': 213, 'email': 214, 'implica': 215, 'dezvoltarea': 216, 'limbajul': 217, 'lista': 218, 'include': 219, 'folosit': 220, 'urmare': 221, 'urmatorul': 222, 'ci': 223, 'parte': 224, 'comunitate': 225, 'skill-uri': 226, 'primi': 227, 'tehnice.': 228, 'internship-ului,': 229, 'evolutia': 230, 'job.': 231, 'ori': 232, 'aplicatie': 233, 'zero': 234, 'impreuna': 235, 'alti': 236, 'colegi': 237, 'invata': 238, 'lucrezi': 239, 'eficient': 240, 'intr-o': 241, 'gasesti': 242, 'problemele': 243, 'tehnice,': 244, 'colaborezi': 245, 'mentorii,': 246, 'pune': 247, 'notiunile': 248, 'teoretice': 249, 'dinainte': 250, 'tine': 251, 'absolventi': 252, 'facultate,': 253, 'an': 254, 'challenge,': 255, 'stii': 256, 'nimic': 257, 'software,': 258, 'incepe': 259, 'bazele': 260, 'structuri': 261, 'date,': 262, 'algoritmi': 263, 'baze': 264, 'date.': 265, 'apoi': 266, 'aprofundezi.': 267, 'https://assist-software.net/learning-resources.': 268, 'suficient': 269, 'completezi': 270, 'formularul': 271, 'necesare': 272, 'acte': 273, 'inscriere.': 274, '1:': 275, '2:': 276, 'evaluarea': 277, 'poate': 278, '3:': 279, 'prezinta': 280, 'nici': 281, 'cost': 282, 'participantilor.': 283, '4:': 284, 'specializare': 285, '5:': 286, 'responsabilitati': 287, 'celor': 288, 'selectati': 289, 'sunt:': 290, 'parcurgerea': 291, 'mentinerea': 292, 'unei': 293, 'linii': 294, 'comunicare': 295, 'activ': 296, 'colaborativ': 297, '6:': 298, 'insa': 299, '7:': 300, '8:': 301, '9:': 302, '10:': 303, 'programul': 304, 'full-time,': 305, 'intervalul': 306, '09:00-18:00,': 307, 'pauza': 308, 'masa': 309, 'ora.': 310, 'program': 311, 'part': 312, 'time,': 313, 'disponibilitatea': 314, 'studentului.': 315, '11:': 316, '12:': 317, 'sfatuim': 318, 'aplici': 319, '01': 320, 'ianuarie': 321, '–': 322, '28': 323, 'februarie,': 324, 'evaluat.': 325, 'timp': 326, 'studierea': 327, 'noi': 328, 'pana': 329, 'inceapa': 330, 'internshipul': 331, 'grup.': 332, '13:': 333, 'conventie': 334, 'universitate': 335, 'participarii': 336, 'echivaleaza': 337, '14:': 338, '15:': 339, 'zile.': 340, '16:': 341, 'stabilit': 342, 'inca': 343, 'datele': 344, 'exacte': 345, 'sesiunilor': 346, 'internship,': 347, 'normal': 348, '17:': 349, 'zile,': 350, '18:': 351, '19:': 352, '20:': 353, 'cei': 354, 'qa': 355, 'manuala': 356, 'automata.': 357, '21:': 358, 'dintre': 359, 'utilizate': 360, 'pregatit': 361, '22:': 362, '23:': 363, '24:': 364, 'doar': 365, 'lunilor': 366, 'august,': 367, 'structurate': 368, 'sesiuni.': 369, '25:': 370, 'prim': 371, 'pas': 372, 'identifici': 373, 'pasioneaza': 374, 'intelegi': 375, 'intersecteaza': 376, 'skill': 377, 'deja,': 378, 'caz': 379, 'continua': 380, 'dezvolti,': 381, 'studiind': 382, 'punand': 383, 'acel': 384, 'skill.': 385, 'descoperit': 386, 'ceva': 387, 'pasioneaza,': 388, 'fa-ti': 389, 'plan': 390, 'includa': 391, 'adauga': 392, 'cv-ului': 393, 'tau.': 394, 'acestea': 395, 'spori': 396, 'sansele': 397, 'domeniul': 398, 'dorit.': 399, 'existenta': 400, 'mentor': 401, 'devii': 402, 'increzator': 403, 'tale,': 404, 'tehnic.': 405, '26:': 406, '27:': 407, '28:': 408, 'participant': 409, 'lucrul': 410, 'evolutie': 411, 'vizibila': 412, '29:': 413, 'persoana': 414, 'cauza': 415, 'trece': 416, 'detine': 417, 'cateva': 418, 'limbaje': 419, 'relevante,': 420, 'lucrat': 421, 'grup/proiecte': 422, 'faca': 423, 'fata': 424, '30:': 425, 'candidat': 426, '31:': 427, 'relevanta': 428, '32:': 429, '33:': 430, 'domeniile': 431, 'interes': 432, 'proprii,': 433, 'incerci': 434, 'curent': 435, 'trend-urile': 436, 'lumea': 437, 'capabil': 438, 'reprofilezi': 439, 'tranzitie': 440, 'usoara': 441, '34:': 442, 'important': 443, 'dezvolti': 444, 'deschis': 445, 'potentiala': 446, 'reprofilare': 447, 'nou.': 448, 'folosesti': 449, 'materialele': 450, 'puse': 451, 'dispozitia': 452, 'catre': 453, 'mentori.': 454, '35:': 455, 'testele': 456, 'create': 457, 'parte,': 458, 'deci': 459, 'rezolva': 460, 'aleasa': 461, 'tine.': 462, '36:': 463, '37:': 464, 'alegi': 465, 'tehnologii': 466, '38:': 467, 'tehnic,': 468, 'ti': 469, 'comunica': 470, 'disponibile': 471, '39:': 472, '40:': 473, 'tip': 474, 'cunostintelor': 475, 'skill-urilor': 476, 'tale': 477, 'tehnologie.': 478, '41:': 479, 'cunosti': 480, 'bine': 481, 'variante': 482, 'tehnologii.': 483, '42:': 484, '43:': 485, 'vara': 486, 'anul': 487, '2021': 488, 's-a': 489, 'swift,': 490, 'departament': 491, 'mobile,': 492, 'format': 493, 'dou': 494, 'ramuri:': 495, 'ios,': 496, 'ramura': 497, 'foloseste': 498, 'swift.': 499, '44:': 500, 'regula': 501, 'timpul': 502, 'ne': 503, 'permite': 504, 'construim': 505, 'zero,': 506, 'folosim': 507, 'existente.': 508, '45:': 509, 'diferit': 510, 'an.': 511, 'vorba': 512, 'impartiti': 513, 'grupe': 514, 'face': 515, 'back-end,': 516, 'front-end,': 517, 'mobile': 518, 'qa.': 519, '46:': 520, 'consideram': 521, 'afectati': 522, 'pandemia': 523, 'covid.': 524, '47:': 525, 'informatii': 526, 'colegilor': 527, 'lucreaza': 528, 'parcurgand': 529, 'articol': 530, 'https://assist-software.net/blog/zero-hero-6-steps-will-guide-you-career-testing.': 531, '48:': 532, 'locuri': 533, 'grafica': 534, '3d/2d,': 535, 'informa': 536, 'intreaga': 537, 'intampla': 538, 'lucru.': 539, '49:': 540, 'te-ar': 541, 'ajuta': 542, 'obtii': 543, 'usurinta': 544, 'domeniu;': 545, 'echipa;': 546, 'detii': 547, 'noi;': 548, 'oportunitatea': 549, 'ajutor': 550, 'sustinere': 551, 'mentori': 552, 'domeniu.': 553, '50:': 554, 'suntem': 555, 'companie': 556, 'succes': 557, 'suceava': 558, 'ofera': 559, 'multiple': 560, 'studentilor': 561, 'cultura': 562, 'inclusiva.': 563, '51:': 564, 'chiar': 565, 'job,': 566, 'randul': 567, 'lor': 568, 'ti-ar': 569, 'oferi': 570, 'set': 571, 'beneficii': 572, 'dezvoltare': 573, 'personala': 574, 'profesionala.': 575, 'dispozitie': 576, 'numeroase': 577, 'materiale': 578, 'cursuri': 579, 'workshop-uri,': 580, 'participa': 581, 'conferinte': 582, 'conventii': 583, '52:': 584, '53:': 585, 'posibilitate': 586, 'mica': 587, 'mare': 588, 'admiterea': 589, '54:': 590, 'principala': 591, 'oportunitate': 592, 'obtinerii': 593, 'companiei.': 594, '55:': 595, 'pasii': 596, 'urmatori': 597, 'acestia:': 598, 'rezultatele': 599, 'bune,': 600, 'individual': 601, '56:': 602, 'buna,': 603, 'atunci': 604, '57:': 605, 'internshipului': 606, '58:': 607, 'etapelor': 608, 'legatura': 609, 'email.': 610, 'nu.': 611, '59:': 612, 'specializari': 613, 'decat': 614, 'calculatoare,': 615, 'automatica': 616, 'informatica,': 617, 'informatica': 618, 'economica.': 619, '60:': 620, '61:': 621, 'per': 622, 'total': 623, 'atat': 624, 'abilitatile': 625, 'soft-skills,': 626, '62:': 627, 'finalul': 628, 'programului': 629, 'adeverinta': 630, 'practica.': 631, '63:': 632, 'multe': 633, 'participi': 634, 'concursurile': 635, 'organizate': 636, 'software': 637, 'studenti:': 638, 'tech': 639, 'coding': 640, 'best': 641, 'innovative': 642, 'minds,': 643, 'urmarind': 644, 'paginile': 645, 'social': 646, 'media.': 647, '64:': 648, 'internshipuri': 649, 'platite.': 650, '65:': 651, 'academy': 652, '66:': 653, '67:': 654, 'limba': 655, 'engleză': 656, 'must-have': 657, 'în': 658, 'industrie.': 659, '68:': 660, 'mentorii': 661, 'colabora': 662, 'membri': 663, 'echipei': 664, 'software.': 665, '69:': 666, 'programele': 667, '70:': 668, 'echipa': 669, 'maxim': 670, '12': 671, 'persoane.': 672, '71:': 673, 'completarea': 674, 'formular': 675, '72:': 676, '73:': 677, '74:': 678, '75:': 679, '76:': 680, '77:': 681, '78:': 682}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'gratis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [102]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEste internship-ul gratis? \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [100]\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(sentence):\n\u001b[0;32m---> 52\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[1;32m     54\u001b[0m   result \u001b[38;5;241m=\u001b[39m targ_lang\u001b[38;5;241m.\u001b[39msequences_to_texts(result)\n",
      "Input \u001b[0;32mIn [100]\u001b[0m, in \u001b[0;36mevaluate_sentence\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     10\u001b[0m text \u001b[38;5;241m=\u001b[39m stemming(text)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(inp_lang\u001b[38;5;241m.\u001b[39mword_index)\n\u001b[0;32m---> 14\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [inp_lang\u001b[38;5;241m.\u001b[39mword_index[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m text]\n\u001b[1;32m     15\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39msequence\u001b[38;5;241m.\u001b[39mpad_sequences([inputs],\n\u001b[1;32m     16\u001b[0m                                                         maxlen\u001b[38;5;241m=\u001b[39mmax_length_input,\n\u001b[1;32m     17\u001b[0m                                                         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(inputs)\n",
      "Input \u001b[0;32mIn [100]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m text \u001b[38;5;241m=\u001b[39m stemming(text)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(inp_lang\u001b[38;5;241m.\u001b[39mword_index)\n\u001b[0;32m---> 14\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [\u001b[43minp_lang\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m text]\n\u001b[1;32m     15\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39msequence\u001b[38;5;241m.\u001b[39mpad_sequences([inputs],\n\u001b[1;32m     16\u001b[0m                                                         maxlen\u001b[38;5;241m=\u001b[39mmax_length_input,\n\u001b[1;32m     17\u001b[0m                                                         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(inputs)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'gratis'"
     ]
    }
   ],
   "source": [
    "predict('Este internship-ul gratis? ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ana\n",
      "are\n",
      "mrere\n"
     ]
    }
   ],
   "source": [
    "sentence = 'ana are mrere'\n",
    "for i in sentence.split(' '):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b750cac4311e9fd618add7f135fe3e519a4aa0ff674a9c2788ad127b0f6cf49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
